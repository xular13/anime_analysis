{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging to log to both console and file\n",
    "LOG_FILE = \"scraping_pipeline.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Log to console\n",
    "        logging.FileHandler(LOG_FILE, mode=\"w\", encoding=\"utf-8\"),  # Log to file\n",
    "    ],\n",
    ")\n",
    "\n",
    "#Constants\n",
    "# Get the MAL_CLIENT_ID from myanimelist.net \n",
    "# Ensure the MAL_CLIENT_ID environment variable is set\n",
    "# Get the MAL_CLIENT_ID from myanimelist.net \n",
    "\n",
    "CLIENT_ID = os.getenv(\"MAL_CLIENT_ID\")\n",
    "if not CLIENT_ID:\n",
    "    raise ValueError(\"MAL_CLIENT_ID environment variable not set!\")\n",
    "BASE_URL = 'https://api.myanimelist.net/v2'\n",
    "HEADERS = {'X-MAL-CLIENT-ID': CLIENT_ID}\n",
    "BASE_PATH = ''\n",
    "CLUBS_FILE = os.path.join(BASE_PATH, 'clubs.txt')\n",
    "USERS_FILE = os.path.join(BASE_PATH, 'users.csv')\n",
    "ANIME_LIST_FILE = os.path.join(BASE_PATH, 'animelist.csv')\n",
    "ANIME_DETAILS_FILE = os.path.join(BASE_PATH, 'anime_details.csv')\n",
    "FAILED_ANIMES = os.path.join(BASE_PATH, 'failed_animes.csv')\n",
    "\n",
    "# Ensure data directory exists if needed \n",
    "# os.makedirs(BASE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url, params=None, retries=3):\n",
    "    \"\"\"Fetch data from a URL with retries for transient errors.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            logging.debug(f\"Fetching data from {url} (Attempt {attempt + 1}/{retries})\")\n",
    "            response = requests.get(url, headers=HEADERS, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            if response.status_code == 500:  # Retry for server errors\n",
    "                logging.warning(f\"500 Server Error on attempt {attempt + 1}/{retries}: {http_err}\")\n",
    "                time.sleep(60)  # Wait before retrying\n",
    "            elif response.status_code == 404:  # Skip 404 errors\n",
    "                logging.error(f\"404 Not Found: {url}\")\n",
    "            elif response.status_code == 403: # Skip 403 errors\n",
    "                logging.warning(f\"403 Forbidden: Access denied for {url}. Skipping retries.\")\n",
    "                return None\n",
    "            else:\n",
    "                logging.error(f\"HTTP error occurred: {http_err}\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException as req_err:\n",
    "            logging.error(f\"Request error occurred: {req_err}\")\n",
    "            time.sleep(60)\n",
    "    logging.error(f\"Failed to fetch data after {retries} retries: {url}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_club_ids():\n",
    "    \"\"\"Scrape club IDs from MyAnimeList.\"\"\"\n",
    "    clubs = set()\n",
    "    page = 1\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    logging.info(\"Starting to scrape club IDs...\")\n",
    "    \n",
    "    while len(clubs) < 300:  # Adjust threshold as needed\n",
    "        logging.info(f\"Scraping club page {page}...\")\n",
    "        url = f\"https://myanimelist.net/clubs.php?sort=5&p={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        rows = soup.find_all(\"tr\", {\"class\": \"table-data\"})\n",
    "\n",
    "        for row in rows:\n",
    "            club_id = int(row.find(\"a\", {\"class\": \"fw-b\"})[\"href\"].split(\"=\")[-1])\n",
    "            clubs.add(club_id)\n",
    "        page += 1\n",
    "\n",
    "    with open(CLUBS_FILE, \"w\") as file:\n",
    "        for club in clubs:\n",
    "            file.write(f\"{club}\\n\")\n",
    "    logging.info(f\"Scraped {len(clubs)} clubs. Saved to {CLUBS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_username_from_club(club_id):\n",
    "    page = 1\n",
    "    users = []\n",
    "    url = f\"https://api.jikan.moe/v4/clubs/{club_id}/members\"\n",
    "    while True:\n",
    "        logging.info(f\"Scraping members of club {club_id} page : {page}...\")\n",
    "        params = {'page' : page}\n",
    "        data = fetch_data(url=url, params=params)\n",
    "        if data and \"data\" in data:\n",
    "            users.extend(user[\"username\"] for user in data[\"data\"])\n",
    "        if data and \"pagination\" in data and data['pagination']['has_next_page'] == True:\n",
    "                page += 1  \n",
    "        else:\n",
    "            break\n",
    "    logging.info(f\"Finished scraping members of club {club_id}...\")\n",
    "    return users\n",
    "\n",
    "\n",
    "def scrape_usernames_from_clubs():\n",
    "    \"\"\"Scrape usernames of members from clubs using threading.\"\"\"\n",
    "    logging.info(\"Starting to scrape usernames from clubs...\")\n",
    "    all_users = []\n",
    "    with open(CLUBS_FILE) as file:\n",
    "        club_ids = [line.strip() for line in file]\n",
    "\n",
    "    for club_id in club_ids:\n",
    "        all_users.extend(fetch_username_from_club(club_id))\n",
    "    users = set(all_users)\n",
    "\n",
    "    with open(USERS_FILE, \"w\") as file:\n",
    "        file.write(\"user_id,username\\n\")\n",
    "        for user_id, username in enumerate(users):\n",
    "            file.write(f\"{user_id},{username}\\n\")\n",
    "    logging.info(f\"Scraped {len(users)} usernames. Saved to {USERS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_user_list(username):\n",
    "    \"\"\"Fetch a single user's anime list, handling pagination.\"\"\"\n",
    "    anime_list = []\n",
    "    offset = 0\n",
    "    url = f\"{BASE_URL}/users/{username}/animelist\"\n",
    "    logging.info(f'Scraping animelist of user {username}...')\n",
    "    # Uncomment the following lines if you want to handle pagination for users with bigger than 1000 entries in list\n",
    "    # while True:\n",
    "    #     params = {'fields': 'list_status', 'offset': offset, 'limit': '1000', 'sort': 'list_score'}\n",
    "        \n",
    "    #     data = fetch_data(url, params)\n",
    "\n",
    "    #     # Break if no data (e.g., 403 or empty response)\n",
    "    #     if not data or \"data\" not in data:\n",
    "    #         break\n",
    "\n",
    "    #     # Process anime list entries\n",
    "    #     for anime in data[\"data\"]:\n",
    "    #         list_status = anime.get(\"list_status\", {})\n",
    "    #         anime_list.append([\n",
    "    #             username,\n",
    "    #             anime[\"node\"][\"id\"],\n",
    "    #             list_status.get(\"status\", \"unknown\"),\n",
    "    #             list_status.get(\"score\", 0),\n",
    "    #             list_status.get(\"num_episodes_watched\", 0)\n",
    "    #         ])\n",
    "\n",
    "        # Check for additional pages\n",
    "        # if \"paging\" in data and \"next\" in data[\"paging\"]:\n",
    "        #     offset += 1000  \n",
    "        # else:\n",
    "        #     break\n",
    "    params = {'fields': 'list_status', 'offset': offset, 'limit': '500', 'sort': 'list_score'}\n",
    "        \n",
    "    data = fetch_data(url, params)\n",
    "\n",
    "    # Break if no data (e.g., 403 or empty response)\n",
    "    if not data or \"data\" not in data:\n",
    "        return None\n",
    "\n",
    "    # Process anime list entries\n",
    "    for anime in data[\"data\"]:\n",
    "        list_status = anime.get(\"list_status\", {})\n",
    "        anime_list.append([\n",
    "            username,\n",
    "            anime[\"node\"][\"id\"],\n",
    "            list_status.get(\"status\", \"unknown\"),\n",
    "            list_status.get(\"score\", 0),\n",
    "            list_status.get(\"num_episodes_watched\", 0)\n",
    "        ])\n",
    "\n",
    "    return anime_list\n",
    "    \n",
    "    \n",
    "def scrape_users_anime_lists():\n",
    "    \"\"\"Scrape anime lists for each user.\"\"\"\n",
    "    logging.info(\"Starting to scrape user anime lists...\")\n",
    "    users = pd.read_csv(USERS_FILE)\n",
    "    all_anime_list = []\n",
    "    for _, row in users.iterrows():\n",
    "        username = row[\"username\"]\n",
    "        data = fetch_user_list(username=username)\n",
    "        if data:\n",
    "            all_anime_list.extend(data)\n",
    "        if row['user_id'] % 10000 == 0: # If there are more than half million of nonzero scores, stop scraping\n",
    "            column_4 = np.array([row[3] for row in all_anime_list], dtype=int)\n",
    "            # Count nonzero values\n",
    "            count_nonzero = np.count_nonzero(column_4)\n",
    "            if count_nonzero >= 500000:\n",
    "                num_of_scrapped = row['user_id']\n",
    "                break\n",
    "\n",
    "    df = pd.DataFrame(all_anime_list, columns=[\"user_id\", \"anime_id\", \"status\", \"score\", \"num_episodes_watched\"])\n",
    "    df.to_csv(ANIME_LIST_FILE, index=False)\n",
    "    logging.info(f\"Scraped anime lists for {num_of_scrapped} users. Saved to {ANIME_LIST_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_anime_details():\n",
    "    \"\"\"Fetch detailed information for each anime.\"\"\"\n",
    "    logging.info(\"Starting to scrape detailed anime information...\")\n",
    "    anime_list = pd.read_csv(ANIME_LIST_FILE)\n",
    "    anime_ids = anime_list[\"anime_id\"].unique()\n",
    "    anime_data = []\n",
    "    failed_animes = []\n",
    "    len_all = len(anime_ids)\n",
    "    start_time = time.time()  # Record the start time\n",
    "    check_interval = 10  # Update speed log every 10 requests\n",
    "    last_update_time = start_time\n",
    "    parsed_count = 0 \n",
    "    params = {'fields': 'id,title,alternative_titles,start_date,end_date,synopsis,mean,rank,popularity,'\n",
    "        'num_list_users,num_scoring_users,nsfw,genres,media_type,status,num_episodes,start_season,'\n",
    "        'source,studios,related_anime'}\n",
    "    \n",
    "    for i, anime_id in enumerate(anime_ids):\n",
    "        url = f\"{BASE_URL}/anime/{anime_id}\"\n",
    "        parsed_count += 1\n",
    "        try:\n",
    "            data = fetch_data(url, params = params)\n",
    "            if data:\n",
    "                anime_data.append(data)\n",
    "                logging.info(f\"Fetched details for anime ID {anime_id}. {i} of {len_all}\")\n",
    "            else:\n",
    "                logging.warning(f\"Failed to fetch details for anime ID {anime_id}. {i} of {len_all}\")\n",
    "                failed_animes.append(anime_id)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to fetch details for anime ID {anime_id}: {e}. {i} of {len_all}\")\n",
    "            failed_animes.append(anime_id)\n",
    "        if (i + 1) % check_interval == 0:  # Update speed log every `check_interval` requests\n",
    "            elapsed_time = time.time() - start_time  # Total elapsed time in seconds\n",
    "            elapsed_minutes = elapsed_time / 60  # Convert to minutes\n",
    "            speed = parsed_count / elapsed_minutes if elapsed_minutes > 0 else 0  # Anime per minute\n",
    "\n",
    "            logging.info(f\"🔥 Speed: {speed:.2f} anime/min ({parsed_count} parsed in {elapsed_minutes:.2f} min)\")\n",
    "\n",
    "            last_update_time = time.time()  # Reset last update time\n",
    "\n",
    "    df = pd.DataFrame(anime_data)\n",
    "    \n",
    "    df.to_csv(ANIME_DETAILS_FILE, index=False, encoding='utf-8')\n",
    "    logging.info(f\"Scraped details for {len(anime_data)} of {len_all} animes. Saved to {ANIME_DETAILS_FILE}\")\n",
    "    \n",
    "    with open(FAILED_ANIMES, \"w\") as file: #Saving failed to parse due to external factors animes id for later retry \n",
    "        file.write(\"count,anime_id\\n\")\n",
    "        for count, anime_id in enumerate(failed_animes):\n",
    "            file.write(f\"{count},{anime_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_anime_details():\n",
    "    \"\"\"Scraping of failed animes.\"\"\"\n",
    "    logging.info(\"Starting to scrape detailed anime information...\")\n",
    "    anime_list = pd.read_csv(FAILED_ANIMES)\n",
    "    anime_ids = anime_list[\"anime_id\"].unique()\n",
    "    anime_data = []\n",
    "    failed_animes = []\n",
    "    len_all = len(anime_ids)\n",
    "    start_time = time.time()  # Record the start time\n",
    "    check_interval = 10  # Update speed log every 10 requests\n",
    "    last_update_time = start_time\n",
    "    parsed_count = 0 \n",
    "    params = {'fields': 'id,title,alternative_titles,start_date,end_date,synopsis,mean,rank,popularity,'\n",
    "        'num_list_users,num_scoring_users,nsfw,genres,media_type,status,num_episodes,start_season,'\n",
    "        'source,studios,related_anime'}\n",
    "    \n",
    "    for i, anime_id in enumerate(anime_ids):\n",
    "        url = f\"{BASE_URL}/anime/{anime_id}\"\n",
    "        parsed_count += 1\n",
    "        try:\n",
    "            data = fetch_data(url, params = params)\n",
    "            if data:\n",
    "                anime_data.append(data)\n",
    "                logging.info(f\"Fetched details for anime ID {anime_id}. {i} of {len_all}\")\n",
    "            else:\n",
    "                logging.warning(f\"Failed to fetch details for anime ID {anime_id}. {i} of {len_all}\")\n",
    "                failed_animes.append(anime_id)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to fetch details for anime ID {anime_id}: {e}. {i} of {len_all}\")\n",
    "            failed_animes.append(anime_id)\n",
    "        if (i + 1) % check_interval == 0:  # Update speed log every `check_interval` requests\n",
    "            elapsed_time = time.time() - start_time  # Total elapsed time in seconds\n",
    "            elapsed_minutes = elapsed_time / 60  # Convert to minutes\n",
    "            speed = parsed_count / elapsed_minutes if elapsed_minutes > 0 else 0  # Anime per minute\n",
    "\n",
    "            logging.info(f\"🔥 Speed: {speed:.2f} anime/min ({parsed_count} parsed in {elapsed_minutes:.2f} min)\")\n",
    "\n",
    "            last_update_time = time.time()  # Reset last update time\n",
    "\n",
    "    df = pd.DataFrame(anime_data)\n",
    "    \n",
    "    \n",
    "    # df.to_csv(ANIME_DETAILS_FILE, index=False, encoding='utf-8')\n",
    "    logging.info(f\"Scraped details for {len(anime_data)} of {len_all} animes. Saved to {ANIME_DETAILS_FILE}\")\n",
    "    return df\n",
    "    \n",
    "    with open(FAILED_ANIMES, \"w\") as file:\n",
    "        file.write(\"count,anime_id\\n\")\n",
    "        for count, anime_id in enumerate(failed_animes):\n",
    "            file.write(f\"{count},{anime_id}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.info(\"Starting the scraping pipeline...\")\n",
    "scrape_club_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_usernames_from_clubs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_users_anime_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_anime_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are failed animes, retry fetching their details\n",
    "df = failed_anime_details()\n",
    "df.to_csv(os.path.join(BASE_PATH, 'anime_details_f.csv'), index=False, encoding='utf-8')\n",
    "df_old = pd.read_csv(ANIME_DETAILS_FILE)\n",
    "frames = [df, df_old]\n",
    "result = pd.concat(frames)\n",
    "result.to_csv(ANIME_DETAILS_FILE, index=False, encoding='utf-8')\n",
    "logging.info(\"Failed anime details scraping completed successfully.\")\n",
    "logging.info(\"All tasks completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to download your data from myanimelist, you can use the following command:\n",
    "username = 'your_username_here'  # Replace with your MyAnimeList username\n",
    "df = pd.DataFrame(fetch_user_list(username), columns=[\"user_id\", \"anime_id\", \"status\", \"score\", \"num_episodes_watched\"])\n",
    "df.to_csv(os.path.join(BASE_PATH, f'your_animelist.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
