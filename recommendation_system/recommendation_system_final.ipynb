{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd13a0bf",
   "metadata": {},
   "source": [
    "# Creating and training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a26fe94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### If you want to train model with your user run cells up-to underline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990da7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'PATH_TO_INPUT_DIRECTORY'  # Replace with your input directory path\n",
    "\n",
    "# Load ratings data\n",
    "ratings_df = pd.read_csv(\n",
    "    f'{INPUT_DIR}/animelist.csv',\n",
    "    usecols=['user_id', 'anime_id', 'score'],\n",
    "    dtype={'user_id': 'int32', 'anime_id': 'int32', 'score': 'float32'}\n",
    ")\n",
    "\n",
    "# Load anime details\n",
    "anime_df = pd.read_csv(\n",
    "    f'{INPUT_DIR}/anime_details.csv',\n",
    "    usecols=['id', 'title', 'synopsis', 'genres', 'mean', 'media_type', 'english_title'],\n",
    "    dtype={'id': 'int32', 'mean': 'float32'}\n",
    ")\n",
    "\n",
    "print(\"Ratings columns:\", ratings_df.columns.tolist())\n",
    "print(\"Anime columns:\", anime_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab40554",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_r = pd.read_csv(f'{INPUT_DIR}/your_animelist.csv',\n",
    "                     usecols=['user_id', 'anime_id', 'score'],\n",
    "                     dtype={'user_id': 'string', 'anime_id': 'int32', 'score': 'float32'})\n",
    "user_r.head()\n",
    "\n",
    "mp = {\n",
    "    'your_nickname': ratings_df['user_id'].max() + 1 # Replace 'your_nickname' with the actual nickname\n",
    "    # Add more mappings if needed\n",
    "}\n",
    "user_r['user_id'] = user_r['user_id'].map(mp)\n",
    "\n",
    "bfr_shape = ratings_df.shape[0]\n",
    "ratings_df = pd.concat([ratings_df, user_r], ignore_index=True)\n",
    "aftr_shape = ratings_df.shape[0]\n",
    "print(f'Before: {bfr_shape}, After: {aftr_shape}, Diff: {aftr_shape - bfr_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83518e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter users/animes with sufficient interactions if needed \n",
    "MIN_RATINGS_PER_USER = 40\n",
    "MIN_RATINGS_PER_ANIME = 10\n",
    "\n",
    "# Filter users\n",
    "user_counts = ratings_df['user_id'].value_counts()\n",
    "ratings_df = ratings_df[ratings_df['user_id'].isin(user_counts[user_counts >= MIN_RATINGS_PER_USER].index)]\n",
    "\n",
    "# Filter animes\n",
    "anime_counts = ratings_df['anime_id'].value_counts()\n",
    "ratings_df = ratings_df[ratings_df['anime_id'].isin(anime_counts[anime_counts >= MIN_RATINGS_PER_ANIME].index)]\n",
    "\n",
    "print(\"Filtered ratings shape:\", ratings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667744c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split first to prevent data leakage\n",
    "train_df, test_df = train_test_split(\n",
    "    ratings_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=ratings_df['user_id']  # Maintain user distribution\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Test size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37369a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize scores to [0, 1] range\n",
    "# Calculate min/max from TRAINING set only\n",
    "train_min = train_df['score'].min()\n",
    "train_max = train_df['score'].max()\n",
    "\n",
    "# Scale ratings to [0, 1]\n",
    "train_df['score_normalized'] = (train_df['score'] - train_min) / (train_max - train_min)\n",
    "test_df['score_normalized'] = (test_df['score'] - train_min) / (train_max - train_min)\n",
    "\n",
    "print(\"Train min/max:\", train_min, train_max)\n",
    "print(\"Scaled train sample:\", train_df['score_normalized'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings\n",
    "user_ids = train_df['user_id'].unique()\n",
    "anime_ids = train_df['anime_id'].unique()\n",
    "\n",
    "user2idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "anime2idx = {anime: idx for idx, anime in enumerate(anime_ids)}\n",
    "\n",
    "# Apply encoding\n",
    "train_df['user'] = train_df['user_id'].map(user2idx)\n",
    "train_df['anime'] = train_df['anime_id'].map(anime2idx)\n",
    "test_df = test_df[test_df['user_id'].isin(user2idx.keys()) & test_df['anime_id'].isin(anime2idx.keys())]  # Filter unseen users/animes\n",
    "test_df['user'] = test_df['user_id'].map(user2idx)\n",
    "test_df['anime'] = test_df['anime_id'].map(anime2idx)\n",
    "\n",
    "print(\"Unique users:\", len(user2idx))\n",
    "print(\"Unique animes:\", len(anime2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db76dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.18.0  # Version known to work with Colab TPUs\n",
    "!pip install tensorflow-tpu==2.18.0 --find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "    print(\"TPU detected:\", resolver.master())\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    print(\"Num TPUs:\", len(tf.config.list_logical_devices('TPU')))\n",
    "except ValueError as e:\n",
    "    print(\"TPU initialization failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess genres\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "anime_df['genres_list'] = anime_df['genres'].fillna('').str.split(', ')\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_matrix = mlb.fit_transform(anime_df['genres_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098aaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map genres to aligned array\n",
    "genre_array = np.zeros((len(anime2idx), len(mlb.classes_)))\n",
    "for _, row in anime_df.iterrows():\n",
    "    idx = anime2idx.get(row['id'])\n",
    "    if idx is not None:\n",
    "        genre_array[idx] = mlb.transform([row['genres_list']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff191b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Flatten, LayerNormalization, Concatenate, Dot, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_model_with_genres(num_users, num_items, genre_dim, emb_dim=32):\n",
    "    with strategy.scope():\n",
    "        # Inputs\n",
    "        user_input = Input(shape=(1,), name='user_input')\n",
    "        anime_input = Input(shape=(1,), name='anime_input')\n",
    "        genre_input = Input(shape=(genre_dim,), name='genre_input')\n",
    "\n",
    "        user_emb = Embedding(num_users, emb_dim, name='user_emb')(user_input)\n",
    "        anime_emb = Embedding(num_items, emb_dim, name='anime_emb')(anime_input)\n",
    "\n",
    "        user_bias = Embedding(num_users, 1, name='user_bias')(user_input)\n",
    "        anime_bias = Embedding(num_items, 1, name='anime_bias')(anime_input)\n",
    "\n",
    "        dot = Dot(axes=-1)([user_emb, anime_emb])\n",
    "\n",
    "        # Genre MLP branch\n",
    "        genre_x = Dense(emb_dim, activation='relu')(genre_input)\n",
    "        genre_x = Dropout(0.3)(genre_x)\n",
    "\n",
    "        # Combine anime + genre\n",
    "        merged_anime = Add()([Flatten()(anime_emb), genre_x])\n",
    "        x = Concatenate()([Flatten()(user_emb), merged_anime])\n",
    "\n",
    "        # MLP\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = LayerNormalization()(x)\n",
    "\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = LayerNormalization()(x)\n",
    "\n",
    "        mlp_out = Dense(1)(x)\n",
    "        bias_sum = Add()([Flatten()(user_bias), Flatten()(anime_bias), Flatten()(dot)])\n",
    "        final_output = Add()([mlp_out, bias_sum])\n",
    "\n",
    "        model = Model(inputs=[user_input, anime_input, genre_input], outputs=final_output)\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(1e-3), metrics=['mae'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_with_genres(len(user2idx), len(anime2idx), genre_dim=genre_array.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_with_genres(df, target_col, user2idx, anime2idx, genre_tensor, batch_size):\n",
    "    user_ids = df['user_id'].map(user2idx).values.astype(np.int32)\n",
    "    anime_ids = df['anime_id'].map(anime2idx).values.astype(np.int32)\n",
    "    ratings = df[target_col].values.astype(np.float32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((user_ids, anime_ids, ratings))\n",
    "\n",
    "    def map_fn(user_id, anime_id, rating):\n",
    "        genre_vec = tf.gather(genre_tensor, anime_id)\n",
    "        return {\n",
    "            'user_input': user_id,         # scalar\n",
    "            'anime_input': anime_id,       # scalar\n",
    "            'genre_input': genre_vec       # (num_genres,)\n",
    "        }, rating                          # scalar\n",
    "\n",
    "    ds = ds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7518ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8192\n",
    "target_col = 'score_normalized'\n",
    "genre_tensor = tf.constant(genre_array.astype(np.float32))  # move to Tensor\n",
    "train_ds = make_dataset_with_genres(train_df, target_col, user2idx, anime2idx, genre_tensor, BATCH_SIZE)\n",
    "val_ds   = make_dataset_with_genres(test_df, target_col, user2idx, anime2idx, genre_tensor, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TRAIN ---\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5)\n",
    "]\n",
    "EPOCHS = 10\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAVE ---\n",
    "save_dir = 'PATH_TO_SAVE_DIRECTORY'  # Replace with your save directory path\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model.save(f\"{save_dir}/anime_rec_model_v.keras\")\n",
    "\n",
    "import pickle \n",
    "# Save mappings\n",
    "with open(f\"{save_dir}/user2idx.pkl\", 'wb') as f:\n",
    "    pickle.dump(user2idx, f)\n",
    "with open(f\"{save_dir}/anime2idx.pkl\", 'wb') as f:\n",
    "    pickle.dump(anime2idx, f)\n",
    "    \n",
    "# Save genre mapping\n",
    "np.save(f\"{save_dir}/genre_array.npy\", genre_tensor.numpy())\n",
    "\n",
    "# Save metadata\n",
    "anime_df.to_parquet(f\"{INPUT_DIR}/saved_model/anime_metadata.parquet\")\n",
    "\n",
    "# Save ratings\n",
    "ratings_df.to_parquet(f\"{INPUT_DIR}/saved_model/train_ratings.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12490e6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552627f",
   "metadata": {},
   "source": [
    "# If you want to use already trained model run those cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6f364",
   "metadata": {},
   "source": [
    "**There should be already saved files:**\n",
    "- model.keras model that was downloaded from link or saved from previus step\n",
    "- user2idx.pkl and anime2idx.pkl mappings of our data\n",
    "- genre_tensor.npy genre matrix\n",
    "- anime_metadata.parquet anime metadata for titles\n",
    "- train_raiting.parquet ratings that used train model `optional`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b209599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Connect google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to google drive\n",
    "INPUT_DIR = '/content/drive/MyDrive'\n",
    "\n",
    "# Define the save path\n",
    "save_dir = f\"{INPUT_DIR}/saved_model\"\n",
    "try:\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "    print(\"TPU detected:\", resolver.master())\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    tpu_strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    print(\"Replicas:\", tpu_strategy.num_replicas_in_sync)\n",
    "except ValueError as e:\n",
    "    print(\"TPU initialization failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_anime_for_user_previous_model(user_id, top_k=10, model_path=f'{save_dir}/anime_rec_model_v6.keras', data_dir=save_dir):\n",
    "    \"\"\"\n",
    "    Recommend Top-K anime for a given user based on the genre-aware rating prediction model.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: ID of the user to recommend for\n",
    "    - top_k: number of recommendations to return\n",
    "    - model_path: path to the trained model (Keras .keras file)\n",
    "    - data_dir: directory containing saved mappings and genre data\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame of recommended anime (anime_id, title)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Load the trained model\n",
    "    with tpu_strategy.scope():\n",
    "\n",
    "        model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "    # Load user2idx and anime2idx mappings\n",
    "    with open(f\"{data_dir}/user2idx.pkl\", \"rb\") as f:\n",
    "        user2idx = pickle.load(f)\n",
    "    with open(f\"{data_dir}/anime2idx.pkl\", \"rb\") as f:\n",
    "        anime2idx = pickle.load(f)\n",
    "    anime_df = pd.read_parquet(f\"{data_dir}/anime_metadata.parquet\")\n",
    "\n",
    "    # Load genre matrix \n",
    "    genre_tensor = np.load(f\"{data_dir}/genre_tensor.npy\").astype(np.float32)\n",
    "\n",
    "    # Check if user_id is known\n",
    "    if user_id not in user2idx:\n",
    "        print(f\"User {user_id} not in dataset.\")\n",
    "        return []\n",
    "\n",
    "    user_idx = user2idx[user_id]\n",
    "    all_anime_indices = np.arange(len(anime2idx))\n",
    "\n",
    "    # Prepare inputs\n",
    "    user_input = np.full_like(all_anime_indices, user_idx)\n",
    "    genre_input = genre_tensor\n",
    "\n",
    "    inputs = {\n",
    "        \"user_input\": user_input,\n",
    "        \"anime_input\": all_anime_indices,\n",
    "        \"genre_input\": genre_input,\n",
    "    }\n",
    "\n",
    "    # Predict scores\n",
    "    scores = model.predict(inputs, batch_size=4096, verbose=0).flatten()\n",
    "\n",
    "    # Filter out items already seen by user\n",
    "    try:\n",
    "        train_df = pd.read_parquet(f\"{data_dir}/train_ratings.parquet\")\n",
    "        seen_anime = set(train_df.loc[train_df['user_id'] == user_id, 'anime_id'])\n",
    "        seen_indices = [anime2idx[aid] for aid in seen_anime if aid in anime2idx]\n",
    "        scores[seen_indices] = -np.inf\n",
    "    except Exception as e:\n",
    "        print(\"Warning: Could not filter seen items:\", e)\n",
    "\n",
    "    # Get top-K indices\n",
    "    top_indices = np.argpartition(-scores, top_k)[:top_k]\n",
    "    top_indices = top_indices[np.argsort(-scores[top_indices])]\n",
    "\n",
    "    # Map back to anime IDs and titles\n",
    "    idx2anime = {v: k for k, v in anime2idx.items()}\n",
    "    recommended_ids = [idx2anime[idx] for idx in top_indices]\n",
    "    recommended_titles = anime_df.loc[anime_df['id'].isin(recommended_ids), ['id', 'english_title', 'title']]\n",
    "\n",
    "    return recommended_titles.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommend_anime_for_user_previous_model(user_id='id from data', top_k=20)\n",
    "recommendations.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
